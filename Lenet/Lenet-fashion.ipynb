{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import layers\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion = keras.datasets.fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = fashion.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b , h , w =  train_images.shape\n",
    "train_images = train_images.reshape([b ,h , w ,1 ])\n",
    "train_labels= train_labels.reshape((60000,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lenet :\n",
    "    def __init__(self,images,labels,batch_size):\n",
    "        self.learning_rate = 0.0002\n",
    "        self.batch_size = batch_size\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        \n",
    "    def build_model(self):\n",
    "        self.x = tf.placeholder(tf.float32, shape = [None, 28,28, 1])\n",
    "        self.y = tf.placeholder(tf.uint8, shape = [None, 1])\n",
    "        self.y_label = tf.one_hot(self.y,depth= 10)\n",
    "        self.x_img = tf.image.resize_images(self.x, (32,32))\n",
    "        \n",
    "        \n",
    "    def Network(self, input_):\n",
    "        with tf.variable_scope(\"Lenet\"):\n",
    "            net = layers.conv(input_,filters = 6)\n",
    "            net = tf.nn.relu(net)\n",
    "            net = layers.pool(net)\n",
    "            net = layers.conv(net,16)\n",
    "            net = tf.nn.relu(net)\n",
    "            net = layers.pool(net)\n",
    "\n",
    "            net = tf.layers.flatten(net)\n",
    "            net = layers.dense(net, units= 120, activation= tf.nn.relu)\n",
    "            net = layers.dense(net, units = 84, activation = tf.nn.relu)\n",
    "            net = layers.dense(net, units = 10)\n",
    "\n",
    "        return net\n",
    "    \n",
    "    def loss_op(self):\n",
    "        self.logit = self.Network(self.x_img)\n",
    "        self.loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.logit,\n",
    "                                                                           labels = self.y_label))\n",
    "        self.trainer = tf.train.AdamOptimizer(learning_rate = self.learning_rate).minimize(self.loss)\n",
    "        \n",
    "    def batch(self, input_, batch_size):\n",
    "        batch_data = input_[:batch_size]\n",
    "        inputs = input_[batch_size:]\n",
    "        return batch_data, inputs\n",
    "    \n",
    "    def train(self):\n",
    "        self.build_model()\n",
    "        self.loss_op()\n",
    "        \n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            loss_ = []\n",
    "            init = tf.global_variables_initializer()\n",
    "            sess.run(init)\n",
    "            for step in range(600):\n",
    "                batch_x,self.images = self.batch(self.images,self.batch_size)\n",
    "                batch_y,self.labels = self.batch(self.labels,self.batch_size)\n",
    "                \n",
    "                _, loss_val = sess.run([self.trainer, self.loss], feed_dict={self.x:batch_x,\n",
    "                                                                             self.y: batch_y})\n",
    "                \n",
    "                loss_.append(loss_val)\n",
    "                \n",
    "                print(\"Training_Step :  {}  ,  Loss  :  {} \". format(step, loss_val))\n",
    "                \n",
    "                if not os.path.exists('mnist_fashion_loss'):\n",
    "                    os.makedirs('mnist_fashion_loss')                    \n",
    "                \n",
    "                if step % 100 == 0 :\n",
    "                    plt.plot(loss_)\n",
    "                    plt.savefig('mnist_fashion_loss/{}.jpg'.format(str(step)))\n",
    "     \n",
    "    \n",
    "    def predict(self, x_test):\n",
    "        return sess.run(self.logit, feed_dict = {self.x : x_test})\n",
    "    \n",
    "    def get_accuracy(self, x_test,y_test):\n",
    "        return sess.run(sel)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Lenet(train_images,train_labels,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_Step :  0  ,  Loss  :  26.028676986694336 \n",
      "Training_Step :  1  ,  Loss  :  22.393033981323242 \n",
      "Training_Step :  2  ,  Loss  :  19.430580139160156 \n",
      "Training_Step :  3  ,  Loss  :  15.254537582397461 \n",
      "Training_Step :  4  ,  Loss  :  12.962869644165039 \n",
      "Training_Step :  5  ,  Loss  :  12.812569618225098 \n",
      "Training_Step :  6  ,  Loss  :  13.720004081726074 \n",
      "Training_Step :  7  ,  Loss  :  9.043825149536133 \n",
      "Training_Step :  8  ,  Loss  :  10.525535583496094 \n",
      "Training_Step :  9  ,  Loss  :  8.475568771362305 \n",
      "Training_Step :  10  ,  Loss  :  8.530699729919434 \n",
      "Training_Step :  11  ,  Loss  :  6.373651027679443 \n",
      "Training_Step :  12  ,  Loss  :  5.93971061706543 \n",
      "Training_Step :  13  ,  Loss  :  4.8563032150268555 \n",
      "Training_Step :  14  ,  Loss  :  6.317119121551514 \n",
      "Training_Step :  15  ,  Loss  :  5.039921760559082 \n",
      "Training_Step :  16  ,  Loss  :  3.7475879192352295 \n",
      "Training_Step :  17  ,  Loss  :  4.620516777038574 \n",
      "Training_Step :  18  ,  Loss  :  4.887375354766846 \n",
      "Training_Step :  19  ,  Loss  :  3.6851861476898193 \n",
      "Training_Step :  20  ,  Loss  :  3.285020112991333 \n",
      "Training_Step :  21  ,  Loss  :  3.154229164123535 \n",
      "Training_Step :  22  ,  Loss  :  3.2834713459014893 \n",
      "Training_Step :  23  ,  Loss  :  3.0853304862976074 \n",
      "Training_Step :  24  ,  Loss  :  3.565195083618164 \n",
      "Training_Step :  25  ,  Loss  :  3.060091257095337 \n",
      "Training_Step :  26  ,  Loss  :  2.802913188934326 \n",
      "Training_Step :  27  ,  Loss  :  3.0783324241638184 \n",
      "Training_Step :  28  ,  Loss  :  2.212256669998169 \n",
      "Training_Step :  29  ,  Loss  :  3.2278246879577637 \n",
      "Training_Step :  30  ,  Loss  :  2.678408145904541 \n",
      "Training_Step :  31  ,  Loss  :  2.559636354446411 \n",
      "Training_Step :  32  ,  Loss  :  2.4882407188415527 \n",
      "Training_Step :  33  ,  Loss  :  2.9280097484588623 \n",
      "Training_Step :  34  ,  Loss  :  2.322110176086426 \n",
      "Training_Step :  35  ,  Loss  :  2.5292372703552246 \n",
      "Training_Step :  36  ,  Loss  :  2.3342669010162354 \n",
      "Training_Step :  37  ,  Loss  :  1.9539681673049927 \n",
      "Training_Step :  38  ,  Loss  :  2.343625068664551 \n",
      "Training_Step :  39  ,  Loss  :  2.150336980819702 \n",
      "Training_Step :  40  ,  Loss  :  2.1357009410858154 \n",
      "Training_Step :  41  ,  Loss  :  1.9786467552185059 \n",
      "Training_Step :  42  ,  Loss  :  2.4124820232391357 \n",
      "Training_Step :  43  ,  Loss  :  2.3332741260528564 \n",
      "Training_Step :  44  ,  Loss  :  2.3015565872192383 \n",
      "Training_Step :  45  ,  Loss  :  1.909521460533142 \n",
      "Training_Step :  46  ,  Loss  :  1.5530457496643066 \n",
      "Training_Step :  47  ,  Loss  :  1.8145159482955933 \n",
      "Training_Step :  48  ,  Loss  :  1.5610198974609375 \n",
      "Training_Step :  49  ,  Loss  :  1.5382075309753418 \n",
      "Training_Step :  50  ,  Loss  :  1.5125055313110352 \n",
      "Training_Step :  51  ,  Loss  :  2.1141114234924316 \n",
      "Training_Step :  52  ,  Loss  :  1.8442106246948242 \n",
      "Training_Step :  53  ,  Loss  :  2.081514835357666 \n",
      "Training_Step :  54  ,  Loss  :  1.6532988548278809 \n",
      "Training_Step :  55  ,  Loss  :  1.771399974822998 \n",
      "Training_Step :  56  ,  Loss  :  1.504404902458191 \n",
      "Training_Step :  57  ,  Loss  :  1.8780256509780884 \n",
      "Training_Step :  58  ,  Loss  :  1.6681410074234009 \n",
      "Training_Step :  59  ,  Loss  :  1.9121917486190796 \n",
      "Training_Step :  60  ,  Loss  :  1.6219171285629272 \n",
      "Training_Step :  61  ,  Loss  :  1.5125617980957031 \n",
      "Training_Step :  62  ,  Loss  :  1.970516562461853 \n",
      "Training_Step :  63  ,  Loss  :  1.5385017395019531 \n",
      "Training_Step :  64  ,  Loss  :  2.1063616275787354 \n",
      "Training_Step :  65  ,  Loss  :  1.5076878070831299 \n",
      "Training_Step :  66  ,  Loss  :  1.670456051826477 \n",
      "Training_Step :  67  ,  Loss  :  1.0157824754714966 \n",
      "Training_Step :  68  ,  Loss  :  1.8512279987335205 \n",
      "Training_Step :  69  ,  Loss  :  1.2336846590042114 \n",
      "Training_Step :  70  ,  Loss  :  1.6123085021972656 \n",
      "Training_Step :  71  ,  Loss  :  1.7316631078720093 \n",
      "Training_Step :  72  ,  Loss  :  1.5548702478408813 \n",
      "Training_Step :  73  ,  Loss  :  1.6761895418167114 \n",
      "Training_Step :  74  ,  Loss  :  2.2496213912963867 \n",
      "Training_Step :  75  ,  Loss  :  1.404543161392212 \n",
      "Training_Step :  76  ,  Loss  :  2.202449083328247 \n",
      "Training_Step :  77  ,  Loss  :  1.5208802223205566 \n",
      "Training_Step :  78  ,  Loss  :  1.8660963773727417 \n",
      "Training_Step :  79  ,  Loss  :  1.5745152235031128 \n",
      "Training_Step :  80  ,  Loss  :  1.6366627216339111 \n",
      "Training_Step :  81  ,  Loss  :  1.4386436939239502 \n",
      "Training_Step :  82  ,  Loss  :  1.4874540567398071 \n",
      "Training_Step :  83  ,  Loss  :  1.880388617515564 \n",
      "Training_Step :  84  ,  Loss  :  1.5621190071105957 \n",
      "Training_Step :  85  ,  Loss  :  1.5956796407699585 \n",
      "Training_Step :  86  ,  Loss  :  1.6989874839782715 \n",
      "Training_Step :  87  ,  Loss  :  1.1552311182022095 \n",
      "Training_Step :  88  ,  Loss  :  1.438578486442566 \n",
      "Training_Step :  89  ,  Loss  :  1.6565868854522705 \n",
      "Training_Step :  90  ,  Loss  :  1.2515499591827393 \n",
      "Training_Step :  91  ,  Loss  :  1.5340896844863892 \n",
      "Training_Step :  92  ,  Loss  :  1.3393157720565796 \n",
      "Training_Step :  93  ,  Loss  :  1.4805792570114136 \n",
      "Training_Step :  94  ,  Loss  :  1.5528924465179443 \n",
      "Training_Step :  95  ,  Loss  :  1.1907174587249756 \n",
      "Training_Step :  96  ,  Loss  :  1.8102189302444458 \n",
      "Training_Step :  97  ,  Loss  :  1.1004563570022583 \n",
      "Training_Step :  98  ,  Loss  :  1.0474721193313599 \n",
      "Training_Step :  99  ,  Loss  :  1.4119983911514282 \n",
      "Training_Step :  100  ,  Loss  :  1.1729037761688232 \n",
      "Training_Step :  101  ,  Loss  :  1.6404515504837036 \n",
      "Training_Step :  102  ,  Loss  :  1.7141810655593872 \n",
      "Training_Step :  103  ,  Loss  :  1.3072090148925781 \n",
      "Training_Step :  104  ,  Loss  :  1.2771297693252563 \n",
      "Training_Step :  105  ,  Loss  :  1.623392939567566 \n",
      "Training_Step :  106  ,  Loss  :  1.0683183670043945 \n",
      "Training_Step :  107  ,  Loss  :  1.3444983959197998 \n",
      "Training_Step :  108  ,  Loss  :  1.5184968709945679 \n",
      "Training_Step :  109  ,  Loss  :  1.6704024076461792 \n",
      "Training_Step :  110  ,  Loss  :  1.217055320739746 \n",
      "Training_Step :  111  ,  Loss  :  1.3004413843154907 \n",
      "Training_Step :  112  ,  Loss  :  1.3715789318084717 \n",
      "Training_Step :  113  ,  Loss  :  1.1154189109802246 \n",
      "Training_Step :  114  ,  Loss  :  1.5184669494628906 \n",
      "Training_Step :  115  ,  Loss  :  1.570936679840088 \n",
      "Training_Step :  116  ,  Loss  :  1.280936598777771 \n",
      "Training_Step :  117  ,  Loss  :  1.2063989639282227 \n",
      "Training_Step :  118  ,  Loss  :  1.516696572303772 \n",
      "Training_Step :  119  ,  Loss  :  1.2394617795944214 \n",
      "Training_Step :  120  ,  Loss  :  1.2719628810882568 \n",
      "Training_Step :  121  ,  Loss  :  1.205478549003601 \n",
      "Training_Step :  122  ,  Loss  :  0.7887665629386902 \n",
      "Training_Step :  123  ,  Loss  :  0.9697109460830688 \n",
      "Training_Step :  124  ,  Loss  :  1.417531132698059 \n",
      "Training_Step :  125  ,  Loss  :  1.0368907451629639 \n",
      "Training_Step :  126  ,  Loss  :  1.2748225927352905 \n",
      "Training_Step :  127  ,  Loss  :  1.092099666595459 \n",
      "Training_Step :  128  ,  Loss  :  0.8983655571937561 \n",
      "Training_Step :  129  ,  Loss  :  1.1051795482635498 \n",
      "Training_Step :  130  ,  Loss  :  0.9353263974189758 \n",
      "Training_Step :  131  ,  Loss  :  1.4800786972045898 \n",
      "Training_Step :  132  ,  Loss  :  1.0308775901794434 \n",
      "Training_Step :  133  ,  Loss  :  1.1839804649353027 \n",
      "Training_Step :  134  ,  Loss  :  1.378556728363037 \n",
      "Training_Step :  135  ,  Loss  :  1.6855096817016602 \n",
      "Training_Step :  136  ,  Loss  :  1.3179512023925781 \n",
      "Training_Step :  137  ,  Loss  :  1.1187140941619873 \n",
      "Training_Step :  138  ,  Loss  :  1.1747130155563354 \n",
      "Training_Step :  139  ,  Loss  :  1.486299991607666 \n",
      "Training_Step :  140  ,  Loss  :  0.8709586262702942 \n",
      "Training_Step :  141  ,  Loss  :  1.2563894987106323 \n",
      "Training_Step :  142  ,  Loss  :  1.1644444465637207 \n",
      "Training_Step :  143  ,  Loss  :  1.0519778728485107 \n",
      "Training_Step :  144  ,  Loss  :  1.1183900833129883 \n",
      "Training_Step :  145  ,  Loss  :  1.1081080436706543 \n",
      "Training_Step :  146  ,  Loss  :  0.9784720540046692 \n",
      "Training_Step :  147  ,  Loss  :  1.0737382173538208 \n",
      "Training_Step :  148  ,  Loss  :  1.2349728345870972 \n",
      "Training_Step :  149  ,  Loss  :  1.331060767173767 \n",
      "Training_Step :  150  ,  Loss  :  1.2332843542099 \n",
      "Training_Step :  151  ,  Loss  :  1.1323035955429077 \n",
      "Training_Step :  152  ,  Loss  :  1.3448725938796997 \n",
      "Training_Step :  153  ,  Loss  :  0.9559508562088013 \n",
      "Training_Step :  154  ,  Loss  :  1.2865135669708252 \n",
      "Training_Step :  155  ,  Loss  :  0.9380766153335571 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_Step :  156  ,  Loss  :  1.1319221258163452 \n",
      "Training_Step :  157  ,  Loss  :  1.098129153251648 \n",
      "Training_Step :  158  ,  Loss  :  0.9513238668441772 \n",
      "Training_Step :  159  ,  Loss  :  1.4289277791976929 \n",
      "Training_Step :  160  ,  Loss  :  1.1397066116333008 \n",
      "Training_Step :  161  ,  Loss  :  1.0249624252319336 \n",
      "Training_Step :  162  ,  Loss  :  1.202580213546753 \n",
      "Training_Step :  163  ,  Loss  :  1.2067705392837524 \n",
      "Training_Step :  164  ,  Loss  :  0.8325197100639343 \n",
      "Training_Step :  165  ,  Loss  :  1.0636932849884033 \n",
      "Training_Step :  166  ,  Loss  :  1.422532081604004 \n",
      "Training_Step :  167  ,  Loss  :  1.4158531427383423 \n",
      "Training_Step :  168  ,  Loss  :  1.1649116277694702 \n",
      "Training_Step :  169  ,  Loss  :  0.7691511511802673 \n",
      "Training_Step :  170  ,  Loss  :  0.9451407790184021 \n",
      "Training_Step :  171  ,  Loss  :  0.8173994421958923 \n",
      "Training_Step :  172  ,  Loss  :  1.5140172243118286 \n",
      "Training_Step :  173  ,  Loss  :  1.0274817943572998 \n",
      "Training_Step :  174  ,  Loss  :  1.4865219593048096 \n",
      "Training_Step :  175  ,  Loss  :  0.9829774498939514 \n",
      "Training_Step :  176  ,  Loss  :  1.2430511713027954 \n",
      "Training_Step :  177  ,  Loss  :  1.129267692565918 \n",
      "Training_Step :  178  ,  Loss  :  1.1580528020858765 \n",
      "Training_Step :  179  ,  Loss  :  0.8517191410064697 \n",
      "Training_Step :  180  ,  Loss  :  1.2229245901107788 \n",
      "Training_Step :  181  ,  Loss  :  0.9957907795906067 \n",
      "Training_Step :  182  ,  Loss  :  0.9742802977561951 \n",
      "Training_Step :  183  ,  Loss  :  0.9178072214126587 \n",
      "Training_Step :  184  ,  Loss  :  0.8896095156669617 \n",
      "Training_Step :  185  ,  Loss  :  1.0841012001037598 \n",
      "Training_Step :  186  ,  Loss  :  0.9178311228752136 \n",
      "Training_Step :  187  ,  Loss  :  1.0037298202514648 \n",
      "Training_Step :  188  ,  Loss  :  0.9934578537940979 \n",
      "Training_Step :  189  ,  Loss  :  0.9959495663642883 \n",
      "Training_Step :  190  ,  Loss  :  1.4978594779968262 \n",
      "Training_Step :  191  ,  Loss  :  1.271155595779419 \n",
      "Training_Step :  192  ,  Loss  :  1.0321365594863892 \n",
      "Training_Step :  193  ,  Loss  :  1.2244278192520142 \n",
      "Training_Step :  194  ,  Loss  :  0.9545631408691406 \n",
      "Training_Step :  195  ,  Loss  :  0.9931486248970032 \n",
      "Training_Step :  196  ,  Loss  :  0.7602734565734863 \n",
      "Training_Step :  197  ,  Loss  :  0.8843367695808411 \n",
      "Training_Step :  198  ,  Loss  :  1.4244734048843384 \n",
      "Training_Step :  199  ,  Loss  :  1.3683538436889648 \n",
      "Training_Step :  200  ,  Loss  :  0.7955530285835266 \n",
      "Training_Step :  201  ,  Loss  :  1.1099053621292114 \n",
      "Training_Step :  202  ,  Loss  :  0.9714184403419495 \n",
      "Training_Step :  203  ,  Loss  :  1.0282678604125977 \n",
      "Training_Step :  204  ,  Loss  :  0.8004370927810669 \n",
      "Training_Step :  205  ,  Loss  :  1.1614104509353638 \n",
      "Training_Step :  206  ,  Loss  :  1.2839350700378418 \n",
      "Training_Step :  207  ,  Loss  :  0.8593698143959045 \n",
      "Training_Step :  208  ,  Loss  :  0.9257568120956421 \n",
      "Training_Step :  209  ,  Loss  :  0.9504475593566895 \n",
      "Training_Step :  210  ,  Loss  :  0.9726200103759766 \n",
      "Training_Step :  211  ,  Loss  :  0.9688289165496826 \n",
      "Training_Step :  212  ,  Loss  :  0.9163494110107422 \n",
      "Training_Step :  213  ,  Loss  :  0.8941680192947388 \n",
      "Training_Step :  214  ,  Loss  :  0.8984129428863525 \n",
      "Training_Step :  215  ,  Loss  :  0.683623731136322 \n",
      "Training_Step :  216  ,  Loss  :  1.038832426071167 \n",
      "Training_Step :  217  ,  Loss  :  0.9739280939102173 \n",
      "Training_Step :  218  ,  Loss  :  1.3683979511260986 \n",
      "Training_Step :  219  ,  Loss  :  0.8460314273834229 \n",
      "Training_Step :  220  ,  Loss  :  0.8965277075767517 \n",
      "Training_Step :  221  ,  Loss  :  1.3132303953170776 \n",
      "Training_Step :  222  ,  Loss  :  1.1131908893585205 \n",
      "Training_Step :  223  ,  Loss  :  1.0021541118621826 \n",
      "Training_Step :  224  ,  Loss  :  0.649071216583252 \n",
      "Training_Step :  225  ,  Loss  :  0.9225925207138062 \n",
      "Training_Step :  226  ,  Loss  :  0.7161273956298828 \n",
      "Training_Step :  227  ,  Loss  :  0.8376916646957397 \n",
      "Training_Step :  228  ,  Loss  :  0.7178063988685608 \n",
      "Training_Step :  229  ,  Loss  :  0.9793025255203247 \n",
      "Training_Step :  230  ,  Loss  :  1.000188946723938 \n",
      "Training_Step :  231  ,  Loss  :  0.9064505100250244 \n",
      "Training_Step :  232  ,  Loss  :  0.8906337022781372 \n",
      "Training_Step :  233  ,  Loss  :  0.8654161095619202 \n",
      "Training_Step :  234  ,  Loss  :  1.1964341402053833 \n",
      "Training_Step :  235  ,  Loss  :  1.1374220848083496 \n",
      "Training_Step :  236  ,  Loss  :  0.8455028533935547 \n",
      "Training_Step :  237  ,  Loss  :  0.9245620965957642 \n",
      "Training_Step :  238  ,  Loss  :  0.6914291977882385 \n",
      "Training_Step :  239  ,  Loss  :  0.9870071411132812 \n",
      "Training_Step :  240  ,  Loss  :  1.084342360496521 \n",
      "Training_Step :  241  ,  Loss  :  0.9216453433036804 \n",
      "Training_Step :  242  ,  Loss  :  0.9493008255958557 \n",
      "Training_Step :  243  ,  Loss  :  0.9445147514343262 \n",
      "Training_Step :  244  ,  Loss  :  1.1247576475143433 \n",
      "Training_Step :  245  ,  Loss  :  1.0664176940917969 \n",
      "Training_Step :  246  ,  Loss  :  0.7702593207359314 \n",
      "Training_Step :  247  ,  Loss  :  1.0290522575378418 \n",
      "Training_Step :  248  ,  Loss  :  1.0734355449676514 \n",
      "Training_Step :  249  ,  Loss  :  1.0694301128387451 \n",
      "Training_Step :  250  ,  Loss  :  0.7880094051361084 \n",
      "Training_Step :  251  ,  Loss  :  0.8127102851867676 \n",
      "Training_Step :  252  ,  Loss  :  0.8389464020729065 \n",
      "Training_Step :  253  ,  Loss  :  0.6470468640327454 \n",
      "Training_Step :  254  ,  Loss  :  0.6537348031997681 \n",
      "Training_Step :  255  ,  Loss  :  0.9663151502609253 \n",
      "Training_Step :  256  ,  Loss  :  0.8155534863471985 \n",
      "Training_Step :  257  ,  Loss  :  0.8536853194236755 \n",
      "Training_Step :  258  ,  Loss  :  0.9434877634048462 \n",
      "Training_Step :  259  ,  Loss  :  0.979753315448761 \n",
      "Training_Step :  260  ,  Loss  :  0.9866068959236145 \n",
      "Training_Step :  261  ,  Loss  :  0.9467830061912537 \n",
      "Training_Step :  262  ,  Loss  :  0.9966912865638733 \n",
      "Training_Step :  263  ,  Loss  :  0.9438823461532593 \n",
      "Training_Step :  264  ,  Loss  :  1.197096586227417 \n",
      "Training_Step :  265  ,  Loss  :  0.41552624106407166 \n",
      "Training_Step :  266  ,  Loss  :  0.6327300071716309 \n",
      "Training_Step :  267  ,  Loss  :  0.7475907206535339 \n",
      "Training_Step :  268  ,  Loss  :  0.6328941583633423 \n",
      "Training_Step :  269  ,  Loss  :  0.8517674803733826 \n",
      "Training_Step :  270  ,  Loss  :  0.7428389191627502 \n",
      "Training_Step :  271  ,  Loss  :  0.7595024704933167 \n",
      "Training_Step :  272  ,  Loss  :  0.7812700867652893 \n",
      "Training_Step :  273  ,  Loss  :  0.8807693719863892 \n",
      "Training_Step :  274  ,  Loss  :  0.9893395304679871 \n",
      "Training_Step :  275  ,  Loss  :  0.9290493726730347 \n",
      "Training_Step :  276  ,  Loss  :  1.260485291481018 \n",
      "Training_Step :  277  ,  Loss  :  0.7664825320243835 \n",
      "Training_Step :  278  ,  Loss  :  0.6583690047264099 \n",
      "Training_Step :  279  ,  Loss  :  0.7611849904060364 \n",
      "Training_Step :  280  ,  Loss  :  1.1200528144836426 \n",
      "Training_Step :  281  ,  Loss  :  0.7463021278381348 \n",
      "Training_Step :  282  ,  Loss  :  1.1101031303405762 \n",
      "Training_Step :  283  ,  Loss  :  0.8415265083312988 \n",
      "Training_Step :  284  ,  Loss  :  0.8980296850204468 \n",
      "Training_Step :  285  ,  Loss  :  0.650175929069519 \n",
      "Training_Step :  286  ,  Loss  :  0.767555296421051 \n",
      "Training_Step :  287  ,  Loss  :  0.579008936882019 \n",
      "Training_Step :  288  ,  Loss  :  0.8179673552513123 \n",
      "Training_Step :  289  ,  Loss  :  0.7436212301254272 \n",
      "Training_Step :  290  ,  Loss  :  0.9612268209457397 \n",
      "Training_Step :  291  ,  Loss  :  1.095191240310669 \n",
      "Training_Step :  292  ,  Loss  :  0.719214677810669 \n",
      "Training_Step :  293  ,  Loss  :  0.8962719440460205 \n",
      "Training_Step :  294  ,  Loss  :  1.050401210784912 \n",
      "Training_Step :  295  ,  Loss  :  0.9060305953025818 \n",
      "Training_Step :  296  ,  Loss  :  0.7055149078369141 \n",
      "Training_Step :  297  ,  Loss  :  0.9871583580970764 \n",
      "Training_Step :  298  ,  Loss  :  0.6915454268455505 \n",
      "Training_Step :  299  ,  Loss  :  1.0044870376586914 \n",
      "Training_Step :  300  ,  Loss  :  0.8245854377746582 \n",
      "Training_Step :  301  ,  Loss  :  0.799344003200531 \n",
      "Training_Step :  302  ,  Loss  :  0.8770700693130493 \n",
      "Training_Step :  303  ,  Loss  :  0.6722425222396851 \n",
      "Training_Step :  304  ,  Loss  :  0.8297353386878967 \n",
      "Training_Step :  305  ,  Loss  :  0.6211157441139221 \n",
      "Training_Step :  306  ,  Loss  :  1.2004728317260742 \n",
      "Training_Step :  307  ,  Loss  :  0.7308376431465149 \n",
      "Training_Step :  308  ,  Loss  :  0.7523305416107178 \n",
      "Training_Step :  309  ,  Loss  :  1.0197930335998535 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_Step :  310  ,  Loss  :  0.6524592041969299 \n",
      "Training_Step :  311  ,  Loss  :  0.6826662421226501 \n",
      "Training_Step :  312  ,  Loss  :  0.8960003852844238 \n",
      "Training_Step :  313  ,  Loss  :  0.7512832880020142 \n",
      "Training_Step :  314  ,  Loss  :  0.9134933352470398 \n",
      "Training_Step :  315  ,  Loss  :  0.7794277667999268 \n",
      "Training_Step :  316  ,  Loss  :  1.0315865278244019 \n",
      "Training_Step :  317  ,  Loss  :  1.0214262008666992 \n",
      "Training_Step :  318  ,  Loss  :  0.7114418148994446 \n",
      "Training_Step :  319  ,  Loss  :  0.5328312516212463 \n",
      "Training_Step :  320  ,  Loss  :  0.8618398904800415 \n",
      "Training_Step :  321  ,  Loss  :  0.7650271058082581 \n",
      "Training_Step :  322  ,  Loss  :  0.7298662066459656 \n",
      "Training_Step :  323  ,  Loss  :  0.7309231758117676 \n",
      "Training_Step :  324  ,  Loss  :  0.8947535753250122 \n",
      "Training_Step :  325  ,  Loss  :  0.5529820322990417 \n",
      "Training_Step :  326  ,  Loss  :  0.6552385091781616 \n",
      "Training_Step :  327  ,  Loss  :  0.9057555198669434 \n",
      "Training_Step :  328  ,  Loss  :  0.8558164238929749 \n",
      "Training_Step :  329  ,  Loss  :  1.0319263935089111 \n",
      "Training_Step :  330  ,  Loss  :  0.748802900314331 \n",
      "Training_Step :  331  ,  Loss  :  0.6636022329330444 \n",
      "Training_Step :  332  ,  Loss  :  1.0280964374542236 \n",
      "Training_Step :  333  ,  Loss  :  0.652326226234436 \n",
      "Training_Step :  334  ,  Loss  :  0.8810790777206421 \n",
      "Training_Step :  335  ,  Loss  :  1.0745759010314941 \n",
      "Training_Step :  336  ,  Loss  :  0.7752801775932312 \n",
      "Training_Step :  337  ,  Loss  :  1.1509380340576172 \n",
      "Training_Step :  338  ,  Loss  :  0.5935861468315125 \n",
      "Training_Step :  339  ,  Loss  :  0.6184362173080444 \n",
      "Training_Step :  340  ,  Loss  :  1.038404941558838 \n",
      "Training_Step :  341  ,  Loss  :  0.8092871904373169 \n",
      "Training_Step :  342  ,  Loss  :  0.8315065503120422 \n",
      "Training_Step :  343  ,  Loss  :  0.8230569362640381 \n",
      "Training_Step :  344  ,  Loss  :  0.7489748597145081 \n",
      "Training_Step :  345  ,  Loss  :  0.631943941116333 \n",
      "Training_Step :  346  ,  Loss  :  0.7700921893119812 \n",
      "Training_Step :  347  ,  Loss  :  0.8558772206306458 \n",
      "Training_Step :  348  ,  Loss  :  0.9932371377944946 \n",
      "Training_Step :  349  ,  Loss  :  1.1905226707458496 \n",
      "Training_Step :  350  ,  Loss  :  0.6722524762153625 \n",
      "Training_Step :  351  ,  Loss  :  0.8668385148048401 \n",
      "Training_Step :  352  ,  Loss  :  0.6386318802833557 \n",
      "Training_Step :  353  ,  Loss  :  0.9764840602874756 \n",
      "Training_Step :  354  ,  Loss  :  0.6907681822776794 \n",
      "Training_Step :  355  ,  Loss  :  1.0757300853729248 \n",
      "Training_Step :  356  ,  Loss  :  0.9769746661186218 \n",
      "Training_Step :  357  ,  Loss  :  0.9242833852767944 \n",
      "Training_Step :  358  ,  Loss  :  0.8816145062446594 \n",
      "Training_Step :  359  ,  Loss  :  0.7875823378562927 \n",
      "Training_Step :  360  ,  Loss  :  1.000883936882019 \n",
      "Training_Step :  361  ,  Loss  :  0.7431747913360596 \n",
      "Training_Step :  362  ,  Loss  :  0.9890336394309998 \n",
      "Training_Step :  363  ,  Loss  :  0.7856439352035522 \n",
      "Training_Step :  364  ,  Loss  :  0.8599370718002319 \n",
      "Training_Step :  365  ,  Loss  :  0.7691448330879211 \n",
      "Training_Step :  366  ,  Loss  :  0.8869936466217041 \n",
      "Training_Step :  367  ,  Loss  :  1.0283918380737305 \n",
      "Training_Step :  368  ,  Loss  :  0.7786208391189575 \n",
      "Training_Step :  369  ,  Loss  :  0.660747766494751 \n",
      "Training_Step :  370  ,  Loss  :  0.8437238931655884 \n",
      "Training_Step :  371  ,  Loss  :  0.8354654908180237 \n",
      "Training_Step :  372  ,  Loss  :  0.8795273303985596 \n",
      "Training_Step :  373  ,  Loss  :  0.7241703867912292 \n",
      "Training_Step :  374  ,  Loss  :  0.7585809230804443 \n",
      "Training_Step :  375  ,  Loss  :  0.6145734190940857 \n",
      "Training_Step :  376  ,  Loss  :  0.7933533191680908 \n",
      "Training_Step :  377  ,  Loss  :  0.687201976776123 \n",
      "Training_Step :  378  ,  Loss  :  0.7503997087478638 \n",
      "Training_Step :  379  ,  Loss  :  1.0361636877059937 \n",
      "Training_Step :  380  ,  Loss  :  0.6886918544769287 \n",
      "Training_Step :  381  ,  Loss  :  0.8275089859962463 \n",
      "Training_Step :  382  ,  Loss  :  0.8397045731544495 \n",
      "Training_Step :  383  ,  Loss  :  0.6061989068984985 \n",
      "Training_Step :  384  ,  Loss  :  0.6534650325775146 \n",
      "Training_Step :  385  ,  Loss  :  0.7511222958564758 \n",
      "Training_Step :  386  ,  Loss  :  0.7769231200218201 \n",
      "Training_Step :  387  ,  Loss  :  0.6393594145774841 \n",
      "Training_Step :  388  ,  Loss  :  0.8446056842803955 \n",
      "Training_Step :  389  ,  Loss  :  0.6378877758979797 \n",
      "Training_Step :  390  ,  Loss  :  0.9230487942695618 \n",
      "Training_Step :  391  ,  Loss  :  0.6990876197814941 \n",
      "Training_Step :  392  ,  Loss  :  0.7927137613296509 \n",
      "Training_Step :  393  ,  Loss  :  0.6842530965805054 \n",
      "Training_Step :  394  ,  Loss  :  0.6406046152114868 \n",
      "Training_Step :  395  ,  Loss  :  0.5094500184059143 \n",
      "Training_Step :  396  ,  Loss  :  0.6246209740638733 \n",
      "Training_Step :  397  ,  Loss  :  0.6236721873283386 \n",
      "Training_Step :  398  ,  Loss  :  0.46601325273513794 \n",
      "Training_Step :  399  ,  Loss  :  0.6147689819335938 \n",
      "Training_Step :  400  ,  Loss  :  0.7092503309249878 \n",
      "Training_Step :  401  ,  Loss  :  1.0174202919006348 \n",
      "Training_Step :  402  ,  Loss  :  0.5596089363098145 \n",
      "Training_Step :  403  ,  Loss  :  0.685667097568512 \n",
      "Training_Step :  404  ,  Loss  :  0.7290785312652588 \n",
      "Training_Step :  405  ,  Loss  :  0.812986433506012 \n",
      "Training_Step :  406  ,  Loss  :  0.7779008746147156 \n",
      "Training_Step :  407  ,  Loss  :  0.6513878703117371 \n",
      "Training_Step :  408  ,  Loss  :  0.9766600131988525 \n",
      "Training_Step :  409  ,  Loss  :  0.6560473442077637 \n",
      "Training_Step :  410  ,  Loss  :  0.7809493541717529 \n",
      "Training_Step :  411  ,  Loss  :  0.7718889713287354 \n",
      "Training_Step :  412  ,  Loss  :  0.6331775784492493 \n",
      "Training_Step :  413  ,  Loss  :  0.8953623175621033 \n",
      "Training_Step :  414  ,  Loss  :  0.6884168386459351 \n",
      "Training_Step :  415  ,  Loss  :  0.7159982323646545 \n",
      "Training_Step :  416  ,  Loss  :  0.7739333510398865 \n",
      "Training_Step :  417  ,  Loss  :  0.5886900424957275 \n",
      "Training_Step :  418  ,  Loss  :  0.6069331169128418 \n",
      "Training_Step :  419  ,  Loss  :  0.8409488797187805 \n",
      "Training_Step :  420  ,  Loss  :  0.9352561235427856 \n",
      "Training_Step :  421  ,  Loss  :  0.9108202457427979 \n",
      "Training_Step :  422  ,  Loss  :  0.6253145337104797 \n",
      "Training_Step :  423  ,  Loss  :  0.9682298302650452 \n",
      "Training_Step :  424  ,  Loss  :  0.6939356923103333 \n",
      "Training_Step :  425  ,  Loss  :  0.6498802900314331 \n",
      "Training_Step :  426  ,  Loss  :  0.6166083812713623 \n",
      "Training_Step :  427  ,  Loss  :  0.8846866488456726 \n",
      "Training_Step :  428  ,  Loss  :  0.6494491100311279 \n",
      "Training_Step :  429  ,  Loss  :  0.6771776080131531 \n",
      "Training_Step :  430  ,  Loss  :  0.785122275352478 \n",
      "Training_Step :  431  ,  Loss  :  0.7526782751083374 \n",
      "Training_Step :  432  ,  Loss  :  0.8380019664764404 \n",
      "Training_Step :  433  ,  Loss  :  0.7493458390235901 \n",
      "Training_Step :  434  ,  Loss  :  0.5205770134925842 \n",
      "Training_Step :  435  ,  Loss  :  0.6546371579170227 \n",
      "Training_Step :  436  ,  Loss  :  0.7449387311935425 \n",
      "Training_Step :  437  ,  Loss  :  0.9125598073005676 \n",
      "Training_Step :  438  ,  Loss  :  0.8810792565345764 \n",
      "Training_Step :  439  ,  Loss  :  0.8048943877220154 \n",
      "Training_Step :  440  ,  Loss  :  0.6625505089759827 \n",
      "Training_Step :  441  ,  Loss  :  0.7015536427497864 \n",
      "Training_Step :  442  ,  Loss  :  0.8478360176086426 \n",
      "Training_Step :  443  ,  Loss  :  0.8842001557350159 \n",
      "Training_Step :  444  ,  Loss  :  0.6611264944076538 \n",
      "Training_Step :  445  ,  Loss  :  0.6368964314460754 \n",
      "Training_Step :  446  ,  Loss  :  0.6485787034034729 \n",
      "Training_Step :  447  ,  Loss  :  0.7269464731216431 \n",
      "Training_Step :  448  ,  Loss  :  0.7351515889167786 \n",
      "Training_Step :  449  ,  Loss  :  0.9167587161064148 \n",
      "Training_Step :  450  ,  Loss  :  0.9789175987243652 \n",
      "Training_Step :  451  ,  Loss  :  0.7808002233505249 \n",
      "Training_Step :  452  ,  Loss  :  0.6637980937957764 \n",
      "Training_Step :  453  ,  Loss  :  0.7576501369476318 \n",
      "Training_Step :  454  ,  Loss  :  0.6413705348968506 \n",
      "Training_Step :  455  ,  Loss  :  0.7400596141815186 \n",
      "Training_Step :  456  ,  Loss  :  0.6283078789710999 \n",
      "Training_Step :  457  ,  Loss  :  0.6800665855407715 \n",
      "Training_Step :  458  ,  Loss  :  0.8345454931259155 \n",
      "Training_Step :  459  ,  Loss  :  0.7115089297294617 \n",
      "Training_Step :  460  ,  Loss  :  0.7219237685203552 \n",
      "Training_Step :  461  ,  Loss  :  0.7721748352050781 \n",
      "Training_Step :  462  ,  Loss  :  0.642396867275238 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training_Step :  463  ,  Loss  :  0.721205472946167 \n",
      "Training_Step :  464  ,  Loss  :  0.5977996587753296 \n",
      "Training_Step :  465  ,  Loss  :  0.6677605509757996 \n",
      "Training_Step :  466  ,  Loss  :  0.9753401279449463 \n",
      "Training_Step :  467  ,  Loss  :  0.5771336555480957 \n",
      "Training_Step :  468  ,  Loss  :  0.8330293297767639 \n",
      "Training_Step :  469  ,  Loss  :  0.792991578578949 \n",
      "Training_Step :  470  ,  Loss  :  0.6623142957687378 \n",
      "Training_Step :  471  ,  Loss  :  1.1013144254684448 \n",
      "Training_Step :  472  ,  Loss  :  0.832237184047699 \n",
      "Training_Step :  473  ,  Loss  :  0.6070457100868225 \n",
      "Training_Step :  474  ,  Loss  :  0.531531810760498 \n",
      "Training_Step :  475  ,  Loss  :  0.9483602046966553 \n",
      "Training_Step :  476  ,  Loss  :  0.7473350763320923 \n",
      "Training_Step :  477  ,  Loss  :  0.5564071536064148 \n",
      "Training_Step :  478  ,  Loss  :  0.5329861640930176 \n",
      "Training_Step :  479  ,  Loss  :  0.6441060900688171 \n",
      "Training_Step :  480  ,  Loss  :  0.6330915689468384 \n",
      "Training_Step :  481  ,  Loss  :  0.5805465579032898 \n",
      "Training_Step :  482  ,  Loss  :  0.6484110951423645 \n",
      "Training_Step :  483  ,  Loss  :  0.48816174268722534 \n",
      "Training_Step :  484  ,  Loss  :  0.5249217748641968 \n",
      "Training_Step :  485  ,  Loss  :  0.7197955250740051 \n",
      "Training_Step :  486  ,  Loss  :  0.6827714443206787 \n",
      "Training_Step :  487  ,  Loss  :  0.8172727823257446 \n",
      "Training_Step :  488  ,  Loss  :  0.46085578203201294 \n",
      "Training_Step :  489  ,  Loss  :  0.8119032382965088 \n",
      "Training_Step :  490  ,  Loss  :  1.1839388608932495 \n",
      "Training_Step :  491  ,  Loss  :  0.5191291570663452 \n",
      "Training_Step :  492  ,  Loss  :  0.4644670784473419 \n",
      "Training_Step :  493  ,  Loss  :  0.6623797416687012 \n",
      "Training_Step :  494  ,  Loss  :  0.721221923828125 \n",
      "Training_Step :  495  ,  Loss  :  0.6754653453826904 \n",
      "Training_Step :  496  ,  Loss  :  0.6946154236793518 \n",
      "Training_Step :  497  ,  Loss  :  0.9600757360458374 \n",
      "Training_Step :  498  ,  Loss  :  0.8255271315574646 \n",
      "Training_Step :  499  ,  Loss  :  0.6881465911865234 \n",
      "Training_Step :  500  ,  Loss  :  0.9261358380317688 \n",
      "Training_Step :  501  ,  Loss  :  0.7175568342208862 \n",
      "Training_Step :  502  ,  Loss  :  1.0176774263381958 \n",
      "Training_Step :  503  ,  Loss  :  0.7053864002227783 \n",
      "Training_Step :  504  ,  Loss  :  0.43078750371932983 \n",
      "Training_Step :  505  ,  Loss  :  0.8957045078277588 \n",
      "Training_Step :  506  ,  Loss  :  0.5841186046600342 \n",
      "Training_Step :  507  ,  Loss  :  0.7050941586494446 \n",
      "Training_Step :  508  ,  Loss  :  0.5846551656723022 \n",
      "Training_Step :  509  ,  Loss  :  0.46363747119903564 \n",
      "Training_Step :  510  ,  Loss  :  0.5856828093528748 \n",
      "Training_Step :  511  ,  Loss  :  0.8843359351158142 \n",
      "Training_Step :  512  ,  Loss  :  0.7951728105545044 \n",
      "Training_Step :  513  ,  Loss  :  0.6609962582588196 \n",
      "Training_Step :  514  ,  Loss  :  0.8310937285423279 \n",
      "Training_Step :  515  ,  Loss  :  0.5287829637527466 \n",
      "Training_Step :  516  ,  Loss  :  0.5273115038871765 \n",
      "Training_Step :  517  ,  Loss  :  0.6437805891036987 \n",
      "Training_Step :  518  ,  Loss  :  0.6005473136901855 \n",
      "Training_Step :  519  ,  Loss  :  0.45120298862457275 \n",
      "Training_Step :  520  ,  Loss  :  0.44097983837127686 \n",
      "Training_Step :  521  ,  Loss  :  0.510459840297699 \n",
      "Training_Step :  522  ,  Loss  :  0.8665468692779541 \n",
      "Training_Step :  523  ,  Loss  :  0.7431049346923828 \n",
      "Training_Step :  524  ,  Loss  :  0.6489563584327698 \n",
      "Training_Step :  525  ,  Loss  :  1.0313935279846191 \n",
      "Training_Step :  526  ,  Loss  :  0.9158816337585449 \n",
      "Training_Step :  527  ,  Loss  :  0.8955846428871155 \n",
      "Training_Step :  528  ,  Loss  :  0.5835937261581421 \n",
      "Training_Step :  529  ,  Loss  :  0.5868473649024963 \n",
      "Training_Step :  530  ,  Loss  :  0.6157352924346924 \n",
      "Training_Step :  531  ,  Loss  :  0.7415054440498352 \n",
      "Training_Step :  532  ,  Loss  :  0.766730546951294 \n",
      "Training_Step :  533  ,  Loss  :  0.765092670917511 \n",
      "Training_Step :  534  ,  Loss  :  0.7879382371902466 \n",
      "Training_Step :  535  ,  Loss  :  0.6745463013648987 \n",
      "Training_Step :  536  ,  Loss  :  0.7198337316513062 \n",
      "Training_Step :  537  ,  Loss  :  0.6810315847396851 \n",
      "Training_Step :  538  ,  Loss  :  0.8312228322029114 \n",
      "Training_Step :  539  ,  Loss  :  0.6483926177024841 \n",
      "Training_Step :  540  ,  Loss  :  0.6709009408950806 \n",
      "Training_Step :  541  ,  Loss  :  0.809416651725769 \n",
      "Training_Step :  542  ,  Loss  :  0.7504314184188843 \n",
      "Training_Step :  543  ,  Loss  :  0.4851776361465454 \n",
      "Training_Step :  544  ,  Loss  :  0.6223336458206177 \n",
      "Training_Step :  545  ,  Loss  :  0.859824538230896 \n",
      "Training_Step :  546  ,  Loss  :  0.538886547088623 \n",
      "Training_Step :  547  ,  Loss  :  0.7262140512466431 \n",
      "Training_Step :  548  ,  Loss  :  0.687604546546936 \n",
      "Training_Step :  549  ,  Loss  :  0.6121814250946045 \n",
      "Training_Step :  550  ,  Loss  :  0.7524808645248413 \n",
      "Training_Step :  551  ,  Loss  :  0.5231942534446716 \n",
      "Training_Step :  552  ,  Loss  :  0.6609123349189758 \n",
      "Training_Step :  553  ,  Loss  :  0.6541941165924072 \n",
      "Training_Step :  554  ,  Loss  :  0.5914762020111084 \n",
      "Training_Step :  555  ,  Loss  :  0.8503085374832153 \n",
      "Training_Step :  556  ,  Loss  :  0.5882413387298584 \n",
      "Training_Step :  557  ,  Loss  :  0.6739909648895264 \n",
      "Training_Step :  558  ,  Loss  :  0.562856912612915 \n",
      "Training_Step :  559  ,  Loss  :  0.6952062249183655 \n",
      "Training_Step :  560  ,  Loss  :  0.6687253713607788 \n",
      "Training_Step :  561  ,  Loss  :  0.6218348741531372 \n",
      "Training_Step :  562  ,  Loss  :  0.4264911413192749 \n",
      "Training_Step :  563  ,  Loss  :  0.7831758856773376 \n",
      "Training_Step :  564  ,  Loss  :  0.6439871788024902 \n",
      "Training_Step :  565  ,  Loss  :  0.6527377367019653 \n",
      "Training_Step :  566  ,  Loss  :  0.6386411190032959 \n",
      "Training_Step :  567  ,  Loss  :  0.5524876713752747 \n",
      "Training_Step :  568  ,  Loss  :  0.6633105278015137 \n",
      "Training_Step :  569  ,  Loss  :  0.8233036994934082 \n",
      "Training_Step :  570  ,  Loss  :  0.708263099193573 \n",
      "Training_Step :  571  ,  Loss  :  0.7897710204124451 \n",
      "Training_Step :  572  ,  Loss  :  0.6918888688087463 \n",
      "Training_Step :  573  ,  Loss  :  0.606570303440094 \n",
      "Training_Step :  574  ,  Loss  :  0.48001593351364136 \n",
      "Training_Step :  575  ,  Loss  :  0.5988901257514954 \n",
      "Training_Step :  576  ,  Loss  :  0.5304774045944214 \n",
      "Training_Step :  577  ,  Loss  :  0.5394490957260132 \n",
      "Training_Step :  578  ,  Loss  :  0.6439541578292847 \n",
      "Training_Step :  579  ,  Loss  :  0.4594120383262634 \n",
      "Training_Step :  580  ,  Loss  :  0.6687151193618774 \n",
      "Training_Step :  581  ,  Loss  :  0.5946688055992126 \n",
      "Training_Step :  582  ,  Loss  :  0.629392147064209 \n",
      "Training_Step :  583  ,  Loss  :  0.7216662764549255 \n",
      "Training_Step :  584  ,  Loss  :  0.7812829613685608 \n",
      "Training_Step :  585  ,  Loss  :  0.4732878804206848 \n",
      "Training_Step :  586  ,  Loss  :  0.593055248260498 \n",
      "Training_Step :  587  ,  Loss  :  0.8049741983413696 \n",
      "Training_Step :  588  ,  Loss  :  0.7723649740219116 \n",
      "Training_Step :  589  ,  Loss  :  0.5054363012313843 \n",
      "Training_Step :  590  ,  Loss  :  0.656725287437439 \n",
      "Training_Step :  591  ,  Loss  :  0.6408658027648926 \n",
      "Training_Step :  592  ,  Loss  :  0.7714508771896362 \n",
      "Training_Step :  593  ,  Loss  :  0.6215410828590393 \n",
      "Training_Step :  594  ,  Loss  :  0.669768750667572 \n",
      "Training_Step :  595  ,  Loss  :  0.5393048524856567 \n",
      "Training_Step :  596  ,  Loss  :  0.7867891192436218 \n",
      "Training_Step :  597  ,  Loss  :  0.6053339838981628 \n",
      "Training_Step :  598  ,  Loss  :  0.7148052453994751 \n",
      "Training_Step :  599  ,  Loss  :  0.6607208847999573 \n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f60e055de48>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
